import os
import requests
import pandas as pd

BASE_URL = "http://kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json"
KEY=os.getenv("MOVIE_KEY")

def gen_url(dt="20120101", url_param={}):
    "í˜¸ì¶œ URL ìƒì„±, url_param ì´ ì…ë ¥ë˜ë©´ multiMovieYn, repNationCd ì²˜ë¦¬"
    url = f"{BASE_URL}?key={KEY}&targetDt={dt}"
    
    # TODO = url_param ì²˜ë¦¬
    for k, v in url_param.items():
        url = url + f"&{k}={v}"
        
    return url

def call_api(dt="20120101", url_param={}):
    url = gen_url(dt, url_param)
    data = requests.get(url)
    j = data.json()
    return j['boxOfficeResult']['dailyBoxOfficeList']

def list2df(data: list, dt: str, url_param={}):
    df = pd.DataFrame(data)
    df['dt'] = dt
    # df['multiMovieYn'] = 'Y'
    for k,v in url_param.items():
        df[k] = v
    
    num_cols = ['rnum', 'rank', 'rankInten', 'salesAmt', 'audiCnt',
                'audiAcc', 'scrnCnt', 'showCnt', 'salesShare', 'salesInten',
                'salesChange', 'audiInten', 'audiChange']

    # for col_name in num_cols:
    #    df[col_name] = pd.to_numeric(df[col_name])
    df[num_cols] = df[num_cols].apply(pd.to_numeric) 

    return df

def save_df(df, base_path, partitions=['dt']):
    df.to_parquet(base_path, partition_cols=partitions)
    save_path = base_path
    for p in partitions:
        save_path = save_path + f"/{p}={df[p][0]}"
    return save_path

def fill_na_with_column(origin_df, c_name):
    df = origin_df.copy()
    for i, row in df.iterrows():
            if pd.isna(row[c_name]):
                same_movie_df = df[df["movieCd"] == row["movieCd"]]
                notna_idx = same_movie_df[c_name].dropna().first_valid_index()
                if notna_idx is not None:
                    df.at[i, c_name] = df.at[notna_idx, c_name]
    return df


def gen_unique(df: pd.DataFrame, drop_columns: list) -> pd.DataFrame:
    df_drop = df.drop(columns=['rnum', 'rank', 'rankInten', 'salesShare','salesChange'])
    unique_df = df_drop.drop_duplicates(subset=['movieCd'])
    return unique_df

def re_ranking(df: pd.DataFrame) -> pd.DataFrame:
    df["rnum"] = df["audiCnt"].rank(method="dense", ascending=False).astype(int)
    df["rank"] = df["audiCnt"].rank(method="min", ascending=False).astype(int)
    return df

def fill_unique_ranking(ds: str, read_base, save_base):
    PATH = f"{read_base}/dt={ds}"
    
    df = pd.read_parquet(PATH)
    df1 = fill_na_with_column(df,'multiMovieYn')
    df2 = fill_na_with_column(df1,'repNationCd')

    drop_columns = ['salesShare','rnum','salesChange','rank','rankInten']
    unique_df = gen_unique(df = df2, drop_columns = drop_columns)
    
    rdf = re_ranking(unique_df)

    rdf['dt'] = ds
    save_path = save_df(rdf, save_base)
    return save_path

def fillna_meta(previous_df, current_df):
    if previous_df is None:
        return current_df  # ì´ì „ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë°ì´í„° ê·¸ëŒ€ë¡œ ë°˜í™˜

    merged_df = current_df.copy()

    # movieCdë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (left join)
    merged_df = merged_df.merge(
        previous_df,
        on="movieCd",
        how="left",
        suffixes=("", "_prev")
    )

    # multiMovieYn ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
    merged_df["multiMovieYn"] = merged_df["multiMovieYn"].fillna(merged_df["multiMovieYn_prev"])

    # repNationCd ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
    merged_df["repNationCd"] = merged_df["repNationCd"].fillna(merged_df["repNationCd_prev"])

    # ë¶ˆí•„ìš”í•œ _prev ì»¬ëŸ¼ ì œê±°
    merged_df.drop(columns=["multiMovieYn_prev", "repNationCd_prev"], inplace=True)

    return merged_df

def load_meta_data(base_path):
    """
    ê¸°ì¡´ ë©”íƒ€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    meta_path = os.path.join(base_path, "meta/meta.parquet")
    return pd.read_parquet(meta_path) if os.path.exists(meta_path) else None


def save_meta_data(base_path, df):
    """
    ë³‘í•©ëœ ë©”íƒ€ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    meta_path = os.path.join(base_path, "meta/meta.parquet")
    os.makedirs(os.path.dirname(meta_path), exist_ok=True)
    df.to_parquet(meta_path)
    return meta_path

def process_meta_data(base_path, ds_nodash):
    """
    ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """
    previous_df = load_meta_data(base_path)

    # ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ
    current_path = os.path.join(base_path, f"dailyboxoffice/dt={ds_nodash}")
    if not os.path.exists(current_path):
        print(f"ğŸš¨ ë°ì´í„° íŒŒì¼ ì—†ìŒ: {current_path}")
        return None

    current_df = pd.read_parquet(current_path)

    # ì´ì „ ë°ì´í„°ì™€ í˜„ì¬ ë°ì´í„° ë³‘í•©í•˜ì—¬ ê²°ì¸¡ì¹˜ ì±„ì›€
    merged_df = fillna_meta(previous_df, current_df)

    # ë³‘í•©ëœ ë°ì´í„°ë¥¼ ë©”íƒ€ ë°ì´í„°ë¡œ ì €ì¥
    save_path = save_meta_data(merged_df, base_path)

    print(f"âœ… ë©”íƒ€ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_path}")
    return merged_df